{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class TFIDFVector(object):\n",
    "    def __init__(self, txt, N, invIndexDict, word_id):\n",
    "        '''\n",
    "        :param str: Orginal String\n",
    "        :param N: The number of total document, a constant\n",
    "        :param invIndexDict: the length of the value for each key is df\n",
    "        :param word_id: vocabulary, given a word, return the id of this word\n",
    "        '''\n",
    "        from collections import defaultdict\n",
    "        import math\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        self.original_str = txt\n",
    "        self.vector = {}\n",
    "        word_count = defaultdict(int)\n",
    "        words = txt.split()\n",
    "        for word in words:\n",
    "            word = lemmatizer.lemmatize(word.lower())\n",
    "            word_count[word] += 1\n",
    "        for word in word_count.keys():\n",
    "            #The vocabular don't have this word\n",
    "            if word not in word_id:\n",
    "                continue\n",
    "            id = word_id[word]\n",
    "            tf = word_count[word]\n",
    "            df = len(invIndexDict[word])\n",
    "            self.vector[id] = (1 + math.log(tf,10)) * math.log(N/df,10)\n",
    "\n",
    "\n",
    "\n",
    "class SearchEngine(object):\n",
    "    def __init__(self, document_num, path=\"./\", dpath=\"./\", opath=\"./\"):\n",
    "        self.path = path\n",
    "        self.dpath = dpath\n",
    "        self.opath = opath\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.doc_tfidf_dict = {}\n",
    "        with open(os.path.join(self.dpath, \"bookkeeping.json\"), 'r') as f:\n",
    "            self.book = json.load(f)\n",
    "        with open(os.path.join(self.path, \"postings.json\"), 'r') as fp:\n",
    "            self.invIndexDict = json.load(fp)\n",
    "        self.N = document_num\n",
    "        self.word_id = {k: v for v, k in enumerate(self.invIndexDict)}\n",
    "\n",
    "\n",
    "    def get_similarity(self, tf_idf_str1, tf_idf_str2):\n",
    "        '''\n",
    "        :param tf_idf_str1:\n",
    "        :param tf_idf_str2:\n",
    "        :return: Cosine similarity of two tf-idf vectors\n",
    "        '''\n",
    "        import math\n",
    "        numerator = 0\n",
    "        for id in tf_idf_str1.vector.keys():\n",
    "            if id in tf_idf_str2.vector.keys():\n",
    "                numerator += tf_idf_str1.vector[id] * tf_idf_str2.vector[id]\n",
    "        norm1 = 0\n",
    "        for id in tf_idf_str1.vector.keys():\n",
    "            norm1 += tf_idf_str1.vector[id]**2\n",
    "        norm1 = math.sqrt(norm1)\n",
    "        norm2 = 0\n",
    "        for id in tf_idf_str2.vector.keys():\n",
    "            norm2 += tf_idf_str2.vector[id]**2\n",
    "        norm2 = math.sqrt(norm2)\n",
    "        return numerator / (norm1 * norm2)\n",
    "\n",
    "    def get_grade(self, query, doc_txt, id):\n",
    "        '''\n",
    "        :param query:\n",
    "        :param doc_txt:\n",
    "        :param id:\n",
    "        :return: grade deault is the similarity between query and doc\n",
    "        If the vector of doc has already saved, retrieve it, else calculate it and save it\n",
    "        '''\n",
    "        tf_idf_query = TFIDFVector(txt=query, N=self.N, invIndexDict=self.invIndexDict, word_id=self.word_id)\n",
    "        tf_idf_doc = None\n",
    "        if id in self.doc_tfidf_dict:\n",
    "            tf_idf_doc = self.doc_tfidf_dict[id]\n",
    "        else:\n",
    "            tf_idf_doc = TFIDFVector(doc_txt,self.N,self.invIndexDict,self.word_id)\n",
    "            self.doc_tfidf_dict[id] = tf_idf_doc\n",
    "\n",
    "        grade = self.get_similarity(tf_idf_query, tf_idf_doc)\n",
    "        return grade\n",
    "\n",
    "\n",
    "    def search(self, query, max_num=10):\n",
    "        #query = lemmatizer.lemmatize(query.lower())\n",
    "        query_words = query.split()\n",
    "        doc_set = set()\n",
    "        for word in query_words:\n",
    "            word = self.lemmatizer.lemmatize(word.lower())\n",
    "            cur_set = set()\n",
    "            for each in self.invIndexDict[word]:\n",
    "                cur_set.add(each[0])\n",
    "            if len(doc_set) == 0:\n",
    "                doc_set = set(cur_set)\n",
    "            else:\n",
    "                doc_set = doc_set.intersection(cur_set)\n",
    "        if len(doc_set) == 0:\n",
    "            print(\"No Result Found!\")\n",
    "            return\n",
    "\n",
    "        grade = {}\n",
    "        for id in doc_set:\n",
    "            dir_id = id.split(\"_\")[0]\n",
    "            doc_id = id.split(\"_\")[1]\n",
    "            file_dir = self.opath + \"/\" + dir_id + \"/\" + doc_id + \".txt\"\n",
    "            doc_txt = \"\"\n",
    "            for line in open(file_dir, \"r\"):\n",
    "                doc_txt += line\n",
    "            grade[id] = self.get_grade(query, doc_txt,id)\n",
    "        #Sort grade\n",
    "        sorted_docs = sorted(grade, key=lambda key: (-grade[key], key))\n",
    "        count = 0\n",
    "        res = []\n",
    "        while count < max_num and count < len(sorted_docs):\n",
    "            id = sorted_docs[count].split(\"_\")[0] + \"/\" + sorted_docs[count].split(\"_\")[1]\n",
    "            res.append(self.book[id])\n",
    "            count += 1\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "N =32044\n",
    "search_engine = SearchEngine(N,path=\"data/INVINDEX\",dpath=\"data/WEBPAGES_RAW\",opath=\"data/WEBPAGES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'www.ics.uci.edu/~irani/options.html',\n",
       " u'www.ics.uci.edu/Arcadia/Teamware/tags/test.html',\n",
       " u'sli.ics.uci.edu/Group/Group?action=login',\n",
       " u'fano.ics.uci.edu/cites/Organization/American-Univ-Dept-of-Computer-Science-+-Information-Systems.html',\n",
       " u'sli.ics.uci.edu/Projects/Projects?action=login',\n",
       " u'sli.ics.uci.edu/Category',\n",
       " u'sli.ics.uci.edu/Code/Code?action=login',\n",
       " u'sli.ics.uci.edu/Notes/Notes?action=login',\n",
       " u'sli.ics.uci.edu/Classes/CS295?action=edit',\n",
       " u'fano.ics.uci.edu/cites/Location/Sequences-II-Communication-Security-and-Computer-Science.html']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_engine.search(\"computer science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'wics.ics.uci.edu/week-5-women-in-technology-talk-by-northrop-grumman',\n",
       " u'wics.ics.uci.edu?p=2280',\n",
       " u'sdcl.ics.uci.edu/2014/06/grant-on-crowd-programming',\n",
       " u'sdcl.ics.uci.edu/2012/02/calico-at-cscw-2012',\n",
       " u'sdcl.ics.uci.edu/2016/05/sdcl-team-in-data-science-hackathon',\n",
       " u'sdcl.ics.uci.edu/author/nmangano',\n",
       " u'www.ics.uci.edu/~kibler/Ethics.htm',\n",
       " u'www.ics.uci.edu/~eppstein/junkyard/bd-deg-tri.html',\n",
       " u'www.ics.uci.edu/community/news/articles/view_article?id=355',\n",
       " u'www.ics.uci.edu/~kobsa/courses/FPS/05S/syllabus.htm']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_engine.search(\"computer sciences happy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opath = \"data/WEBPAGES\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = opath+\"/50\"+\"/\"+\"451.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index of project clonedetection tool\n",
      "\n",
      "index of project clonedetection tool name last modified size description parent directory demo jun latest may source jun apache ubuntu server at mondego ic uci edu port\n"
     ]
    }
   ],
   "source": [
    "for each in open(file_dir,'r'):\n",
    "    print each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade = {\"test\":1,\"d\":3,\"lo\":2,\"pz\":0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade = sorted(grade, key=lambda key: (-grade[key], key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d', 'lo', 'test', 'pz']\n"
     ]
    }
   ],
   "source": [
    "print grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
